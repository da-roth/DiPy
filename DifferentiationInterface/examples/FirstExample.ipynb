{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.069162135812496\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = 1.7\n",
    "a = np.sqrt(2)\n",
    "y = np.exp(x * a)\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-16 17:36:00.256246: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-16 17:36:00.256272: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-16 17:36:00.256760: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-06-16 17:36:00.659324: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.getcwd()  # Check the current working directory\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "import src.diffipy.diffipy as dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp((1.7 * sqrt(2)))\n",
      "11.069162135812496\n"
     ]
    }
   ],
   "source": [
    "import src.diffipy.diffipy as dp\n",
    "\n",
    "x = 1.7\n",
    "a = dp.sqrt(2)\n",
    "y = dp.exp(x * a)\n",
    "\n",
    "print(y)\n",
    "print(y.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp((1.7 * sqrt(2)))\n",
      "11.069162135812496\n"
     ]
    }
   ],
   "source": [
    "import src.diffipy.diffipy as np\n",
    "\n",
    "x = 1.7\n",
    "a = np.sqrt(2)\n",
    "y = np.exp(x * a)\n",
    "\n",
    "print(y)\n",
    "print(y.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backend      Result       Gradient    \n",
      "numpy        11.069162    15.654270   \n"
     ]
    }
   ],
   "source": [
    "x_value = 1.7\n",
    "x= dp.variable(x_value)\n",
    "a = dp.sqrt(2)\n",
    "y = dp.exp(x * a)\n",
    "\n",
    "result = y.eval()\n",
    "derivative = y.grad(x)\n",
    "\n",
    "print(\"{:<12} {:<12} {:<12}\".format('Backend', 'Result', 'Gradient'))\n",
    "print(\"{:<12} {:<12.6f} {:<12.6f}\".format(\"numpy\", result, derivative))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backend      Result       Gradient    \n",
      "torch        11.069163    15.654160   \n"
     ]
    }
   ],
   "source": [
    "dp.setBackend('torch')\n",
    "x_value = 1.7\n",
    "x= dp.variable(x_value)\n",
    "a = dp.sqrt(2)\n",
    "\n",
    "y = dp.exp(x * a)\n",
    "\n",
    "result = y.eval()\n",
    "derivative = y.grad(x)\n",
    "\n",
    "print(\"{:<12} {:<12} {:<12}\".format('Backend', 'Result', 'Gradient'))\n",
    "print(\"{:<12} {:<12.6f} {:<12.6f}\".format(\"torch\", result, derivative))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backend      Result       Gradient    \n",
      "numpy        13.082915    [15.654269908971228, 2.0137627762650823]\n",
      "torch        13.082916    [15.654160499572754, 2.0137526988983154]\n",
      "tensorflow   13.082916    [15.6541605, 2.0137527]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "get_optimized_executable() missing 2 required positional arguments: 'input_dict' and 'diff_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m f \u001b[38;5;241m=\u001b[39m dp\u001b[38;5;241m.\u001b[39mexp(x \u001b[38;5;241m*\u001b[39m a) \u001b[38;5;241m+\u001b[39m dp\u001b[38;5;241m.\u001b[39mexp(y)\n\u001b[1;32m     14\u001b[0m result \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m---> 15\u001b[0m derivative \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{:<12}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{:<12.6f}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{:<12}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(backend, result, \u001b[38;5;28mstr\u001b[39m(derivative)))\n",
      "File \u001b[0;32m~/Documents/dev/DiffiPy/DifferentiationInterface/src/diffipy/Node.py:113\u001b[0m, in \u001b[0;36mNode.grad\u001b[0;34m(self, diffDirection)\u001b[0m\n\u001b[1;32m    111\u001b[0m     diffDirection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_inputs_with_diff()\n\u001b[1;32m    112\u001b[0m instance_grad_class \u001b[38;5;241m=\u001b[39m grad_class(\u001b[38;5;28mself\u001b[39m, diffDirection)\n\u001b[0;32m--> 113\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minstance_grad_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackend_specific_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/dev/DiffiPy/DifferentiationInterface/src/diffipy/backends/jax/NodesJax.py:118\u001b[0m, in \u001b[0;36mDifferentiationNodeJAX.backend_specific_grad\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m#print(input_variables)\u001b[39;00m\n\u001b[1;32m    116\u001b[0m input_dict \u001b[38;5;241m=\u001b[39m {var\u001b[38;5;241m.\u001b[39midentifier: var\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;28;01mfor\u001b[39;00m var \u001b[38;5;129;01min\u001b[39;00m input_variables}\n\u001b[0;32m--> 118\u001b[0m myfunc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moperand\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_optimized_executable\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m result_class \u001b[38;5;241m=\u001b[39m ResultNodeJAX(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    122\u001b[0m _, gradient \u001b[38;5;241m=\u001b[39m result_class\u001b[38;5;241m.\u001b[39meval_and_grad_of_function(myfunc, input_dict, input_dict)\n",
      "\u001b[0;31mTypeError\u001b[0m: get_optimized_executable() missing 2 required positional arguments: 'input_dict' and 'diff_dict'"
     ]
    }
   ],
   "source": [
    "backend_array = ['numpy', 'torch', 'tensorflow', 'jax', 'aadc']\n",
    "print(\"{:<12} {:<12} {:<12}\".format('Backend', 'Result', 'Gradient'))\n",
    "\n",
    "for backend in backend_array:\n",
    "    dp.setBackend(backend)\n",
    "\n",
    "    x_value = 1.7\n",
    "    y_value = 0.7\n",
    "    x = dp.variable(x_value)\n",
    "    y = dp.variable(y_value)\n",
    "    a = dp.sqrt(2)\n",
    "    f = dp.exp(x * a) + dp.exp(y)\n",
    "    \n",
    "    result = f.eval()\n",
    "    derivative = f.grad()\n",
    "\n",
    "    print(\"{:<12} {:<12.6f} {:<12}\".format(backend, result, str(derivative)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backend      Result       Gradient    \n",
      "numpy        13.082915    2.0137627762650823\n",
      "torch        13.082916    2.0137526988983154\n",
      "tensorflow   13.082916    2.0137527   \n",
      "jax          13.082915    2.0137527   \n",
      "You are using evaluation version of AADC. Expire date is 20240901\n",
      "aadc         13.082915    [2.01375271]\n"
     ]
    }
   ],
   "source": [
    "# Differentiation in a specific direction\n",
    "backend_array = ['numpy', 'torch', 'tensorflow', 'jax','aadc']\n",
    "print(\"{:<12} {:<12} {:<12}\".format('Backend', 'Result', 'Gradient'))\n",
    "\n",
    "for backend in backend_array:\n",
    "    dp.setBackend(backend)\n",
    "\n",
    "    x_value = 1.7\n",
    "    y_value = 0.7\n",
    "    x = dp.variable(x_value)\n",
    "    y = dp.variable(y_value)\n",
    "    a = dp.sqrt(2)\n",
    "    f = dp.exp(x * a) + dp.exp(y)\n",
    "    \n",
    "    result = f.eval()\n",
    "    derivative = f.grad(y)\n",
    "\n",
    "    print(\"{:<12} {:<12.6f} {:<12}\".format(backend, result, str(derivative)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
