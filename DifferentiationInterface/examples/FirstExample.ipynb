{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.069162135812496\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = 1.7\n",
    "a = np.sqrt(2)\n",
    "y = np.exp(x * a)\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.getcwd()  # Check the current working directory\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "import src.diffipy.diffipy as dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp((constant(1.7) * sqrt(constant(2))))\n",
      "11.069162368774414\n"
     ]
    }
   ],
   "source": [
    "import src.diffipy.diffipy as dp\n",
    "\n",
    "x = 1.7\n",
    "a = dp.sqrt(2)\n",
    "y = dp.exp(x * a)\n",
    "\n",
    "print(y)\n",
    "print(y.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp((constant(1.7) * sqrt(constant(2))))\n",
      "11.069162368774414\n"
     ]
    }
   ],
   "source": [
    "import src.diffipy.diffipy as np\n",
    "\n",
    "x = 1.7\n",
    "a = np.sqrt(2)\n",
    "y = np.exp(x * a)\n",
    "\n",
    "print(y)\n",
    "print(y.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backend      Result       Gradient    \n",
      "numpy        11.069162    15.654160   \n"
     ]
    }
   ],
   "source": [
    "x_value = 1.7\n",
    "x= dp.variable(x_value)\n",
    "a = dp.sqrt(2)\n",
    "y = dp.exp(x * a)\n",
    "\n",
    "result = y.eval()\n",
    "derivative = y.grad(x)\n",
    "\n",
    "print(\"{:<12} {:<12} {:<12}\".format('Backend', 'Result', 'Gradient'))\n",
    "print(\"{:<12} {:<12.6f} {:<12.6f}\".format(\"numpy\", result, derivative))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backend      Result       Gradient    \n",
      "torch        11.069163    15.654160   \n"
     ]
    }
   ],
   "source": [
    "dp.setBackend('torch')\n",
    "x_value = 1.7\n",
    "x= dp.variable(x_value)\n",
    "a = dp.sqrt(2)\n",
    "\n",
    "y = dp.exp(x * a)\n",
    "\n",
    "result = y.eval()\n",
    "derivative = y.grad(x)\n",
    "\n",
    "print(\"{:<12} {:<12} {:<12}\".format('Backend', 'Result', 'Gradient'))\n",
    "print(\"{:<12} {:<12.6f} {:<12.6f}\".format(\"torch\", result, derivative))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backend      Result       Gradient    \n",
      "numpy        13.082915    [15.654269908971228, 2.0137627762650823]\n",
      "torch        13.082916    [15.654160499572754, 2.0137526988983154]\n",
      "tensorflow   13.082916    [15.6541605, 2.0137527]\n",
      "jax          13.082915    [15.654159545898438, 2.0137526988983154]\n",
      "aadc         13.082915    [array([15.65415922]), array([2.01375271])]\n",
      "You are using evaluation version of AADC. Expire date is 20240901\n"
     ]
    }
   ],
   "source": [
    "backend_array = ['numpy', 'torch', 'tensorflow', 'jax', 'aadc']\n",
    "print(\"{:<12} {:<12} {:<12}\".format('Backend', 'Result', 'Gradient'))\n",
    "\n",
    "for backend in backend_array:\n",
    "    dp.setBackend(backend)\n",
    "\n",
    "    x_value = 1.7\n",
    "    y_value = 0.7\n",
    "    x = dp.variable(x_value)\n",
    "    y = dp.variable(y_value)\n",
    "    a = dp.sqrt(2)\n",
    "    f = dp.exp(x * a) + dp.exp(y)\n",
    "    \n",
    "    result = f.eval()\n",
    "    derivative = f.grad()\n",
    "\n",
    "    print(\"{:<12} {:<12.6f} {:<12}\".format(backend, result, str(derivative)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backend      Result       Gradient    \n",
      "numpy        13.082915    2.0137627762650823\n",
      "torch        13.082916    2.0137526988983154\n",
      "tensorflow   13.082916    2.0137527   \n",
      "jax          13.082915    2.0137527   \n",
      "aadc         13.082915    [2.01375271]\n",
      "You are using evaluation version of AADC. Expire date is 20240901\n"
     ]
    }
   ],
   "source": [
    "# Differentiation in a specific direction\n",
    "backend_array = ['numpy', 'torch', 'tensorflow', 'jax','aadc']\n",
    "print(\"{:<12} {:<12} {:<12}\".format('Backend', 'Result', 'Gradient'))\n",
    "\n",
    "for backend in backend_array:\n",
    "    dp.setBackend(backend)\n",
    "\n",
    "    x_value = 1.7\n",
    "    y_value = 0.7\n",
    "    x = dp.variable(x_value)\n",
    "    y = dp.variable(y_value)\n",
    "    a = dp.sqrt(2)\n",
    "    f = dp.exp(x * a) + dp.exp(y)\n",
    "    \n",
    "    result = f.eval()\n",
    "    derivative = f.grad(y)\n",
    "\n",
    "    print(\"{:<12} {:<12.6f} {:<12}\".format(backend, result, str(derivative)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
